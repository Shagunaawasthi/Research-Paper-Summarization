{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm==4.36.1 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (4.36.1)\n"
     ]
    }
   ],
   "source": [
    "#for progress bar to track time to complete for any given fxn\n",
    "!pip install tqdm==4.36.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores to test accuracy \n",
    "#!pip install rogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRASOON KUMAR\\anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "#numpy array\n",
    "import numpy as np\n",
    "#pandas\n",
    "import pandas as pd\n",
    "#provides functions for interacting with the operating system.\n",
    "import os\n",
    "#serialize the object\n",
    "import pickle\n",
    "#this module let you check if a particular string matches a given regular expression\n",
    "import re\n",
    "#Loading Own Text Data Into Scikit\n",
    "from sklearn.datasets import load_files\n",
    "#the glob module is used to retrieve files/pathnames matching a specified pattern.\n",
    "import glob\n",
    "#stop words\n",
    "from nltk.corpus import stopwords\n",
    "#tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "# to transform a given text into a vector on the basis of the frequency \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#to express the importance of a word to a document \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#prgress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "'''uses top academic models and modern statistical machine learning to perform various\n",
    "complex tasks such as Building document or word vectors, Corpora, performing topic identification, \n",
    "performing document comparison(retrieving semantically similar documents), \n",
    "analysing plain-text documents for semantic structure.'''\n",
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\prasoon kumar\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "#gensim is used for ,\n",
    "#it supports an implementation of the Word2Vec \n",
    "#word embedding for learning new word vectors from text.\n",
    "\n",
    "#It also provides tools for loading pre-trained word embeddings \n",
    "#in a few formats and for making use and querying a loaded embedding.\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "''' Get full path for file `fname` in test data directory placed in this module directory. \n",
    "Usually used to place corpus to test_data directory'''\n",
    "#load glove file of 6 billion tokens and 50 dimensions\n",
    "glove_file = datapath(os.path.abspath(r'C:\\Users\\PRASOON KUMAR\\Research_Me\\Research-Paper-Summarization\\glove.6B.50d.txt'))\n",
    "#This function doesnâ€™t creates file (only generate unique name). \n",
    "#Also, it may return different paths in consecutive calling.\n",
    "#create temp file\n",
    "tmp_file = get_tmpfile(os.path.abspath(\"test_word2vec.txt\"))\n",
    "#to convert the GloVe file format to the word2vec file format. \n",
    "converted_file = glove2word2vec(glove_file, tmp_file) \n",
    "#KeyedVectors.load_word2vec_format() function to load this model into memory\n",
    "glove_model = KeyedVectors.load_word2vec_format(os.path.abspath(\"test_word2vec.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model[\"the\"]  # Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop word removal function\n",
    "import gensim\n",
    "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def stop_word_remove(sentence):\n",
    "    temp = [token for token in sentence.split() if token not in all_stopwords]\n",
    "    return ' '.join(word for word in temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load paper\n",
    "def read_paper(path):\n",
    "    \n",
    "    \n",
    "    f = open(path, 'r', encoding=\"utf-8\")\n",
    "    text = str(f.read())\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bodyMain(text):\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    \n",
    "\n",
    "  # Extracting the highlights, body from the paper\n",
    "    \n",
    "    body_main = re.findall(r'.*(?:abstract)(.*?)references', text, flags=re.I)[0]\n",
    "\n",
    "    body = ' '.join(body_main.split())\n",
    "    body = body.split(\".\")\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    \n",
    "\n",
    "  # Removes unwanted characters, accounting for unicode characters\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    \n",
    "\n",
    "  # Extracting the highlights, body from the paper\n",
    "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    body_main = re.findall(r'.*(?:abstract)(.*?)references', text, flags=re.I)[0]\n",
    "    #back= re.findall(r'.*(?:BACKGROUND)(.*?)references', text, flags=re.I)[0]\n",
    "    meth=re.findall(r'METHOD(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    proc=re.findall(r'PROCEDURE(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    res=re.findall(r'RESULTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    dis=re.findall(r'DISCUSSION(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    con=re.findall(r'CONCLUSIONS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    ack=re.findall(r'ACKNOWLEDGEMENTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    ref=re.findall(r'REFERENCES(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t2 = read_paper(path)\n",
    "#extract(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def process_paper(text):\n",
    "    \n",
    "\n",
    "  # Removes unwanted characters, accounting for unicode characters\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    \n",
    "\n",
    "  # Extracting the highlights, body from the paper\n",
    "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    body_main = re.findall(r'.*(?:abstract)(.*?)references', text, flags=re.I)[0]\n",
    "    \n",
    "    \n",
    "  # Making a copy of the body, lowercasing body text, removing punctuations & extra spaces\n",
    "    dummy_body = body_main.lower()\n",
    "    dummy_body = re.sub('[^\\w\\s\\d\\.]','',dummy_body)\n",
    "    dummy_body = ' '.join(dummy_body.split())\n",
    "    dummy_body = dummy_body.split(\".\")\n",
    "\n",
    "  # Removing extra spaces from the body text, which will be preserved to produce summaries\n",
    "  # And splitting into sentences\n",
    "    body = ' '.join(body_main.split())\n",
    "    body = body.split(\".\")\n",
    "\n",
    "  # Removing sentences that are too short or too long, as they wouldn't make apt summary text\n",
    "    for i,x in enumerate(dummy_body):\n",
    "        if (len(x.split())) < 3 or (len(x.split())) > 15: \n",
    "            \n",
    "            dummy_body.pop(i)\n",
    "            body.pop(i)\n",
    "\n",
    "  # Making a copy of the highlights, lowercasing body text, removing punctuations & extra spaces\n",
    "    dummy_highlights = highlights.lower()\n",
    "    dummy_highlights = re.sub('[^\\w\\s\\d]','',dummy_highlights)\n",
    "    dummy_highlights = ' '.join(dummy_highlights.split())\n",
    "\n",
    "  # Removing stop words from body & highlights\n",
    "    body_copy = []\n",
    "    for x in dummy_body:\n",
    "      \n",
    "        body_copy.append(stop_word_remove(x))\n",
    "    highlight_copy = []\n",
    "    for x in dummy_highlights.split():\n",
    "        highlight_copy.append(stop_word_remove(x))\n",
    "\n",
    "  \n",
    "  # Combing all of the highlights into one string    \n",
    "    highlight_copy = \" \".join(sentence for sentence in highlight_copy)\n",
    "    highlight_copy = \" \".join(highlight_copy.split())\n",
    "    #print(\"BC\",body_copy)\n",
    "    \n",
    "    return body_main, body_copy, highlights, highlight_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "#DOCUMENT SCORE IS CALCULATED USING GLOVE EMBEDDINGS\n",
    "# Function to calculate sentence Score\n",
    "def document_score(body_copy, highlight_copy):\n",
    "    \n",
    "  # Getting word vectors for the body\n",
    "    body_vectors = []\n",
    "    for sent in body_copy:\n",
    "        sent_vec = []\n",
    "        #split word by space\n",
    "        for word in sent.split():\n",
    "            try:\n",
    "                #attach it's embedding if present\n",
    "                sent_vec.append(glove_model[word])\n",
    "          # If the word vector isn't there in the model\n",
    "          # then use the vector of the word \"Visual\"\n",
    "            except:\n",
    "                sent_vec.append(glove_model[\"visual\"])\n",
    "        #append all vectors is body_vector list\n",
    "        body_vectors.append(sent_vec)\n",
    "\n",
    "  # Getting word vectors for the highlights\n",
    "    highlight_vectors = []\n",
    "    for word in highlight_copy.split():\n",
    "        try:\n",
    "            highlight_vectors.append(glove_model[word])\n",
    "        except:\n",
    "            highlight_vectors.append(glove_model[\"visual\"])\n",
    "\n",
    "  # Finding the rouge score for each sentence by counting the # of common words\n",
    "  # & dividing by length of sentence\n",
    "  #highlights are user generated summaries\n",
    "    doc_score = []\n",
    "    for sent in body_vectors:\n",
    "        sent_score = 0\n",
    "        for word in sent:\n",
    "            for w in highlight_vectors:\n",
    "                if (word == w).all():\n",
    "                    sent_score+=1\n",
    "        try: \n",
    "            #sigmoid fxn: 1/(1+exp(-x))\n",
    "            doc_score.append(expit(sent_score/len(sent)))\n",
    "        except:\n",
    "            doc_score.append(0)\n",
    "    return doc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Function to create document vectors\n",
    "def create_document_vector(body_main, doc_score):\n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    doc1 = [body_main]\n",
    "    \n",
    "\n",
    "    # Transforming data\n",
    "    docs = []\n",
    "    '''it supports both access from key value and iteration, the functionality that dictionaries lack.'''\n",
    "    #in order to train the doc2vec model we need to tag tha document.\n",
    "    #or we could also have used models.doc2vec.TaggedDcument()  for tagging the document\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', ['words' ,'tags'])\n",
    "    for i, d in enumerate(doc1):\n",
    "        #we have split all the words in a document and stored them with tag 0\n",
    "        \n",
    "        words = d.lower().split()\n",
    "        \n",
    "        \n",
    "        tags = [i]\n",
    "        \n",
    "        docs.append(analyzedDocument(words, tags))\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "      # Training model\n",
    "    model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "    \n",
    "\n",
    "      # Getting vectors\n",
    "      #using tag 0 to access\n",
    "      #docvecs is used to access vectors generated by the model for the corpus\n",
    "    doc_vec = model.docvecs[0]\n",
    "    \n",
    "\n",
    "    doc_vectors = []\n",
    "    print(\"l\",len(doc_score))\n",
    "    for i in range(len(doc_score)):\n",
    "        doc_vectors.append(doc_vec)\n",
    "    #append all of the vectors in a list\n",
    "    return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sentence vectors\n",
    "def create_sentence_vectors(body_copy):\n",
    "    doc2 = body_copy\n",
    "\n",
    "  # Transforming data \n",
    "    docu = []\n",
    "    analyzed = namedtuple('Analyzed', ['words' ,'tags'])\n",
    "    for i, f in enumerate(doc2):\n",
    "        words = f.split()\n",
    "        tags = [i]\n",
    "        \n",
    "        docu.append(analyzed(words, tags))\n",
    "\n",
    "  # Training model\n",
    "    model = doc2vec.Doc2Vec(docu, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "  # Getting vectors\n",
    "    #raw array of all trained tags: vectors_docs\n",
    "    sent_vectors = model.docvecs.vectors_docs\n",
    "    return list(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create datasets\n",
    "def create_data(path):\n",
    "    text = read_paper(path)\n",
    "    \n",
    "    body_main, body_copy, highlights, highlight_copy = process_paper(text)\n",
    "    \n",
    "    \n",
    "    doc_score = document_score(body_copy, highlight_copy)\n",
    "    doc_vectors = create_document_vector(body_main, doc_score)\n",
    "    sent_vectors = create_sentence_vectors(body_copy)\n",
    "    #print(\"doc_vec\",doc_vectors) \n",
    "    \n",
    "    #print(\"sent_vec\",sent_vectors) \n",
    "    #column wise concatenate\n",
    "    x = np.concatenate([doc_vectors, sent_vectors], axis=1).tolist()\n",
    "    \n",
    "    x = pd.DataFrame(x)\n",
    "    #print(x.head())\n",
    "    y = pd.DataFrame(doc_score)\n",
    "    #print(y.head)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c280ca7da640b8960c8840495b3849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l 205\n",
      "l 111\n",
      "l 194\n",
      "l 234\n",
      "l 264\n",
      "l 126\n",
      "l 160\n",
      "l 142\n",
      "l 108\n",
      "l 121\n",
      "l 32\n",
      "l 188\n",
      "l 132\n",
      "l 187\n",
      "l 7\n",
      "l 104\n",
      "l 178\n",
      "l 214\n",
      "l 168\n",
      "l 118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Create a Gaussian Classifier --- \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "Model = SGDRegressor()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Creating a list of all file paths & partially fitting the model --- \n",
    "\n",
    "paths = glob.glob(r\"C:\\Users\\PRASOON KUMAR\\Research_Me\\Research-Paper-Summarization\\Parsed_Papers\\*.txt\")\n",
    "for i,path in enumerate(tqdm(paths[0:20])):\n",
    "    x, y= create_data(path)\n",
    "    Model.partial_fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l 140\n",
      "        0         1         2        3         4         5         6    \\\n",
      "0  0.575497  0.693983 -0.910348  1.14543 -0.597955 -1.284419  0.528183   \n",
      "1  0.575497  0.693983 -0.910348  1.14543 -0.597955 -1.284419  0.528183   \n",
      "2  0.575497  0.693983 -0.910348  1.14543 -0.597955 -1.284419  0.528183   \n",
      "3  0.575497  0.693983 -0.910348  1.14543 -0.597955 -1.284419  0.528183   \n",
      "4  0.575497  0.693983 -0.910348  1.14543 -0.597955 -1.284419  0.528183   \n",
      "\n",
      "        7         8         9    ...       190       191       192       193  \\\n",
      "0 -0.858224  0.665786 -0.012202  ... -0.001344  0.002423 -0.000517  0.001028   \n",
      "1 -0.858224  0.665786 -0.012202  ... -0.003823 -0.002810 -0.001714 -0.000763   \n",
      "2 -0.858224  0.665786 -0.012202  ...  0.000755 -0.001813  0.002178  0.000099   \n",
      "3 -0.858224  0.665786 -0.012202  ... -0.004260 -0.000006  0.001362 -0.003918   \n",
      "4 -0.858224  0.665786 -0.012202  ...  0.000758  0.000489  0.004733 -0.004238   \n",
      "\n",
      "        194       195       196       197       198       199  \n",
      "0  0.004737  0.000668  0.000474 -0.001956 -0.003162 -0.001284  \n",
      "1  0.002000 -0.002957  0.001435  0.004860 -0.002789 -0.002495  \n",
      "2  0.004678  0.003139  0.000756 -0.004372  0.002375 -0.001362  \n",
      "3 -0.000752  0.004185 -0.002045  0.000135  0.002273  0.001952  \n",
      "4 -0.001692  0.001792 -0.001730 -0.001221 -0.001446  0.000356  \n",
      "\n",
      "[5 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Dummy data --- \n",
    "\n",
    "\n",
    "t, b= create_data(r\"C:\\Users\\PRASOON KUMAR\\Research_Me\\Research-Paper-Summarization\\Parsed_Papers\\S0142694X1500054X.txt\")\n",
    "print(t.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 4, 47, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Predicting the top 4 summary sentences --- \n",
    "\n",
    "c = Model.predict(t)\n",
    "lst = pd.Series(c)\n",
    "i = lst.nlargest(4)\n",
    "i = i.index.values.tolist()\n",
    "i # Indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bodyMain(path):\n",
    "    text = read_paper(path)\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    \n",
    "\n",
    "  # Extracting the highlights, body from the paper\n",
    "    \n",
    "    body_main = re.findall(r'.*(?:abstract)(.*?)references', text, flags=re.I)[0]\n",
    "\n",
    "    body = ' '.join(body_main.split())\n",
    "    body = body.split(\".\")\n",
    "    return body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highLight(path):\n",
    "    text = read_paper(path)\n",
    "    \n",
    "  # Removes unwanted characters, accounting for unicode characters\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    \n",
    "\n",
    "  # Extracting the highlights, body from the paper\n",
    "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    \n",
    "    return highlights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3) (MannWhitney U, p<0',\n",
       " ' A survey of 122 graphic designers and clients identified that these two groups may not be communicating with each other effectively with regard to visual accessibility, and that there is a need to develop inclusive design tools to assist them with this',\n",
       " ' There are a variety of tools and methods to assist designers from all disciplines when considering their users, such as the Inclusive Design Toolkit (Clarkson, Coleman, Hosking, & Waller, 2011), Userfit (Poulson, Ashby, & Richardson, 1996), The Universal Design Handbook (Preiser & Ostroff, 2001) and the methods proposed by Stanford D School (Stanford University, 2014)',\n",
       " 'It is essential that graphic design is visually clear and accessible']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=r\"C:\\Users\\PRASOON KUMAR\\Research_Me\\Research-Paper-Summarization\\Parsed_Papers\\S0142694X1500054X.txt\"\n",
    "\n",
    "body=bodyMain(t)\n",
    "summary = []\n",
    "\n",
    "for x in range(4):\n",
    "    summary.append(body[i[x]])\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                             We conducted a survey of 122 graphic designers and clients.                                                                                                            We investigated the consideration given to visual accessibility in graphic design.                                                                                                            Designers and clients do not communicate effectively about visual accessibility.                                                                                                            We need to develop tools to assist designers with visual accessibility.                                                                                                            We need to investigate the development of ethical design codes.                                                                                                    '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highL=highLight(t)\n",
    "highL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A survey of 122 graphic designers and clients identified that these two groups may not be communicating with each other effectively with regard to visual accessibility, and that there is a need to develop inclusive design tools to assist them with this3) (MannWhitney U, p<0 However, evidence suggests that a lack of consideration is given to visual accessibility in print-based graphic design There are a variety of tools and methods to assist designers from all disciplines when considering their users, such as the Inclusive Design Toolkit (Clarkson, Coleman, Hosking, & Waller, 2011), Userfit (Poulson, Ashby, & Richardson, 1996), The Universal Design Handbook (Preiser & Ostroff, 2001) and the methods proposed by Stanford D School (Stanford University, 2014)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t=\"\"\n",
    "for i in range(len(summary)):\n",
    "    t+=summary[i]\n",
    "    \n",
    "t   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRouge.pyrouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is :0.035440613026819924\n",
      "Recall is :0.37373737373737376\n",
      "F Score is :0.06474289143916452\n"
     ]
    }
   ],
   "source": [
    "r = Rouge()\n",
    "\n",
    "[precision, recall, f_score] = r.rouge_l([highL], [t])\n",
    "\n",
    "print(\"Precision is :\"+str(precision)+\"\\nRecall is :\"+str(recall)+\"\\nF Score is :\"+str(f_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
